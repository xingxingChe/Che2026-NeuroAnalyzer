import os
import pandas as pd
import numpy as np
from scipy.ndimage import gaussian_filter1d
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import FastICA
from itertools import combinations
import tkinter as tk
from tkinter import filedialog

# =============================================================================
# å…¨å±€å‚æ•°è®¾ç½® (Global Parameters)
# =============================================================================
# é™é‡‡æ ·ç›®æ ‡æ”¾ç”µç‡ (Hz) - åŸºäº CUMS ç»„ PYR çš„è¾ƒä½å¹³å‡æ”¾ç”µç‡è®¾å®š
TARGET_PYR_RATE = 6.765 


# =============================================================================
# Helper Functions for Analysis
# =============================================================================

def downsample_spikes(spike_train, target_rate, total_recording_time):
    """
    é€šè¿‡æ— æ”¾å›çš„éšæœºæŠ½æ ·ï¼Œå°†å•ä¸ªç¥ç»å…ƒçš„ Spike train æŠ½ç¨€è‡³ target_rateï¼Œå¹¶è¿”å›é‡æ–°æ’åºåçš„æ—¶é—´æˆ³æ•°ç»„ã€‚
    """
    current_count = len(spike_train)
    if current_count == 0 or total_recording_time <= 0:
        return spike_train

    current_rate = current_count / total_recording_time
    
    # å¦‚æœåŸå§‹æ”¾ç”µç‡å·²ç»ä½äºæˆ–ç­‰äºç›®æ ‡æ”¾ç”µç‡ï¼Œåˆ™ä¸éœ€è¦é™é‡‡æ ·
    if current_rate <= target_rate:
        return spike_train

    target_count = int(target_rate * total_recording_time)
    if target_count <= 0:
        return np.array([])

    # æ— æ”¾å›çš„éšæœºæŠ½æ ·
    sampled_spikes = np.random.choice(spike_train, size=target_count, replace=False)
    # å¿…é¡»å¯¹æŠ½æ ·åçš„æ—¶é—´æˆ³é‡æ–°æ’åºï¼Œä»¥ä¿è¯æ—¶é—´åºåˆ—çš„æ­£ç¡®æ€§
    return np.sort(sampled_spikes)


def detect_assemblies(spike_count_matrix, num_to_extract=2):
    """
    Detects cell assemblies using ICA, with a fixed number of components to extract.
    """
    num_neurons, _ = spike_count_matrix.shape

    if num_neurons < 2:
        return np.array([])

    scaler = StandardScaler()
    z_scored_matrix = scaler.fit_transform(spike_count_matrix.T).T

    n_assemblies = num_to_extract

    if num_neurons < n_assemblies:
        print(f"      [è­¦å‘Š] âš ï¸ ç¥ç»å…ƒæ•°é‡({num_neurons})å°‘äºè¦æå–çš„é›†ç¾¤æ•°({n_assemblies})ã€‚è·³è¿‡æ­¤æ–‡ä»¶çš„é›†ç¾¤æå–ã€‚")
        return np.array([])

    ica = FastICA(n_components=n_assemblies, whiten='unit-variance', max_iter=1000, tol=0.001)
    try:
        ica.fit(z_scored_matrix.T)
    except Exception as e:
        print(f"      [é”™è¯¯] âŒ ICAæ‹Ÿåˆå¤±è´¥: {e}ã€‚è·³è¿‡æ­¤æ–‡ä»¶çš„é›†ç¾¤åˆ†æã€‚")
        return np.array([])

    weight_vectors = ica.components_

    for i in range(n_assemblies):
        vec = weight_vectors[i, :]
        if np.abs(np.min(vec)) > np.max(vec):
            weight_vectors[i, :] = -vec
        norm = np.linalg.norm(vec)
        if norm > 0:
            weight_vectors[i, :] /= np.linalg.norm(weight_vectors[i, :])

    return weight_vectors


def reconstruct_and_quantify_activations(spike_count_matrix, weight_vectors, T_total):
    """
    Reconstructs assembly activations and quantifies their strength and frequency.
    """
    if weight_vectors.shape[0] == 0:
        return np.nan, np.nan

    smoothed_counts = gaussian_filter1d(spike_count_matrix, sigma=1, axis=1)
    scaler = StandardScaler()
    z_scored_smoothed = scaler.fit_transform(smoothed_counts.T).T

    all_strengths, all_frequencies = [], []

    for i in range(weight_vectors.shape[0]):
        w = weight_vectors[i, :]
        P = np.outer(w, w)
        activation_strength_t = np.sum((z_scored_smoothed.T @ P) * z_scored_smoothed.T, axis=1)
        activations = activation_strength_t[activation_strength_t > 5]

        if len(activations) > 0:
            avg_strength = np.mean(activations)
            activation_events = np.sum((activation_strength_t[:-1] <= 5) & (activation_strength_t[1:] > 5))
            frequency = activation_events / T_total
        else:
            avg_strength = np.nan
            frequency = 0.0

        all_strengths.append(avg_strength)
        all_frequencies.append(frequency)

    return np.nanmean(all_strengths), np.nanmean(all_frequencies)


def calculate_pairwise_sync(pyr_spike_times, T_total):
    """
    Calculates normalized pairwise synchrony for all PYR pairs.
    """
    if len(pyr_spike_times) < 2: return []
    neuron_ids = list(pyr_spike_times.keys())
    sync_values = []
    # *** å‚æ•°è°ƒæ•´ï¼šåŒæ­¥çª—å£å¤§å°è°ƒæ•´å›25ms ***
    window_size = 0.025
    frequencies = {nid: len(spikes) / T_total for nid, spikes in pyr_spike_times.items() if T_total > 0}

    for id1, id2 in combinations(neuron_ids, 2):
        spikes1, spikes2 = pyr_spike_times[id1], pyr_spike_times[id2]
        if len(spikes1) == 0 or len(spikes2) == 0: continue

        coincident_spikes = 0
        # A more efficient way to count coincident spikes
        for t1 in spikes1:
            # Find insertion point to quickly check the window
            idx = np.searchsorted(spikes2, t1 - window_size / 2, side='left')
            while idx < len(spikes2) and spikes2[idx] <= t1 + window_size / 2:
                coincident_spikes += 1
                idx += 1

        expected_chance = 2 * frequencies.get(id2, 0) * window_size * len(spikes1)
        if expected_chance > 0:
            sync_values.append(coincident_spikes / expected_chance)
    return sync_values


def calculate_cofiring_validation(spike_count_matrix, weight_vectors):
    """
    Calculates co-firing coefficients (Pearson's r) for high- vs. low-weight neurons.
    """
    if weight_vectors.shape[0] == 0: return []
    validation_data = []
    for asm_idx, w_vec in enumerate(weight_vectors):
        threshold = 2 * np.std(w_vec)
        high_indices = np.where(w_vec > threshold)[0]
        low_indices = np.where(w_vec <= threshold)[0]
        if len(high_indices) >= 2:
            for i, j in combinations(high_indices, 2):
                with np.errstate(divide='ignore', invalid='ignore'):  # Handle constant spike trains
                    corr = np.corrcoef(spike_count_matrix[i, :], spike_count_matrix[j, :])[0, 1]
                if not np.isnan(corr):
                    validation_data.append(
                        {"Assembly_Index": asm_idx, "Weight_Category": "High", "Co-firing_Coefficient_r": corr})
        if len(low_indices) >= 2:
            num_to_sample = len(high_indices) * (len(high_indices) - 1) // 2
            num_to_sample = min(num_to_sample, 100)  # Sample a reasonable number of pairs
            if num_to_sample > 0:
                low_pairs = list(combinations(low_indices, 2))
                sample_indices = np.random.choice(len(low_pairs), min(len(low_pairs), num_to_sample), replace=False)
                for idx in sample_indices:
                    i, j = low_pairs[idx]
                    with np.errstate(divide='ignore', invalid='ignore'):
                        corr = np.corrcoef(spike_count_matrix[i, :], spike_count_matrix[j, :])[0, 1]
                    if not np.isnan(corr):
                        validation_data.append(
                            {"Assembly_Index": asm_idx, "Weight_Category": "Low", "Co-firing_Coefficient_r": corr})
    return validation_data


def process_single_file(file_path, save_fig5b_data=False, output_dir="."):
    """
    Main processing pipeline for a single mouse data file.
    """
    mouse_id = os.path.splitext(os.path.basename(file_path))[0]
    print(f"\n==================================================")
    print(f"ğŸ”„ å¼€å§‹å¤„ç†å°é¼ æ•°æ®: {mouse_id}")
    print(f"==================================================")

    print(f"  -> [1/7] ğŸ“‚ æ­£åœ¨è¯»å– Excel æ•°æ®...")
    df = pd.read_excel(file_path)
    neuron_types = df.iloc[0]
    pyr_columns = neuron_types[neuron_types == 1].index

    if len(pyr_columns) < 2:
        print(f"  -> [è­¦å‘Š] âš ï¸ PYRæ•°é‡({len(pyr_columns)})å°‘äº2ï¼Œæ— æ³•è¿›è¡Œç½‘ç»œåˆ†æï¼Œè·³è¿‡æ­¤æ–‡ä»¶ã€‚")
        return None, None, None

    print(f"  -> [2/7] ğŸ§© æˆåŠŸè¯†åˆ« {len(pyr_columns)} ä¸ªé”¥ä½“ç»†èƒ(PYR)ï¼Œæ­£åœ¨æå– Spike Times...")
    pyr_spike_times = {col: df[col][1:].dropna().to_numpy() for col in pyr_columns}

    T_total = 0
    for spikes in pyr_spike_times.values():
        if len(spikes) > 0 and spikes.max() > T_total:
            T_total = spikes.max()

    # =========================================================================
    # ç¾¤ä½“é™é‡‡æ ·æ§åˆ¶ (Population Down-sampling Control)
    # =========================================================================
    is_ctrl = 'ctrl' in mouse_id.lower()
    print(f"  -> [3/7] âš–ï¸ ç»„åˆ«åˆ¤å®šä¸ç¾¤ä½“é™é‡‡æ ·...")

    if is_ctrl:
        print(f"      âœ… æ£€æµ‹åˆ° Ctrl ç»„æ•°æ®ï¼Œæ­£åœ¨æ‰§è¡Œç¾¤ä½“é™é‡‡æ · (ç›®æ ‡æ”¾ç”µç‡: {TARGET_PYR_RATE} Hz)...")
        downsample_count = 0
        for col in pyr_columns:
            original_spikes = pyr_spike_times[col]
            original_rate = len(original_spikes) / T_total if T_total > 0 else 0
            if original_rate > TARGET_PYR_RATE:
                downsample_count += 1
            downsampled_spikes = downsample_spikes(original_spikes, TARGET_PYR_RATE, T_total)
            pyr_spike_times[col] = downsampled_spikes
        print(f"      âœ… é™é‡‡æ ·å®Œæˆï¼å…±å¯¹ {downsample_count} ä¸ªé«˜æ”¾ç”µç‡ç¥ç»å…ƒè¿›è¡Œäº†æŠ½ç¨€å¤„ç†ã€‚")
    else:
        print(f"      âœ… æ£€æµ‹åˆ°é Ctrl ç»„æ•°æ® (å¦‚ CUMS)ï¼Œè·³è¿‡é™é‡‡æ ·ï¼Œä¿æŒåŸå§‹æ”¾ç”µç‡ã€‚")
    # =========================================================================

    print(f"  -> [4/7] ğŸ“Š æ­£åœ¨åˆ’åˆ†æ—¶é—´çª— (Bin size = 25ms) å¹¶æ„å»º Spike Count Matrix...")
    bin_size = 0.025
    bins = np.arange(0, T_total + bin_size, bin_size)
    spike_count_matrix = np.zeros((len(pyr_columns), len(bins) - 1))
    
    for i, neuron_id in enumerate(pyr_columns):
        spikes = pyr_spike_times[neuron_id]
        if len(spikes) > 0:
            spike_count_matrix[i, :], _ = np.histogram(spikes, bins=bins)

    scaler = StandardScaler()
    z_scored_matrix = scaler.fit_transform(spike_count_matrix.T).T
    covariance_matrix = np.cov(z_scored_matrix)

    print(f"  -> [5/7] ğŸ§  æ­£åœ¨ä½¿ç”¨ FastICA ç®—æ³•æå–ç¥ç»å…ƒåŒæ­¥é›†ç¾¤ (Cell Assemblies)...")
    weight_vectors = detect_assemblies(spike_count_matrix, num_to_extract=2)
    print(f"      âœ… å°è¯•æå– 2 ä¸ªé›†ç¾¤ï¼ŒæˆåŠŸæ‰¾åˆ° {weight_vectors.shape[0]} ä¸ªã€‚")

    if save_fig5b_data and weight_vectors.shape[0] > 0:
        cov_path = os.path.join(output_dir, f"{mouse_id}_covariance_matrix.csv")
        weights_path = os.path.join(output_dir, f"{mouse_id}_weight_vectors.csv")
        np.savetxt(cov_path, covariance_matrix, delimiter=",")
        np.savetxt(weights_path, weight_vectors, delimiter=",")
        print(f"      ğŸ’¾ å·²å°†åæ–¹å·®çŸ©é˜µå’Œæƒé‡å‘é‡ä¿å­˜ç”¨äº Fig.5b ä½œå›¾ã€‚")

    print(f"  -> [6/7] ğŸ“ˆ æ­£åœ¨é‡å»ºé›†ç¾¤æ¿€æ´»è½¨è¿¹ï¼Œå¹¶è®¡ç®—æ¿€æ´»å¼ºåº¦ä¸é¢‘ç‡...")
    avg_strength, avg_frequency = reconstruct_and_quantify_activations(spike_count_matrix, weight_vectors, T_total)
    
    print(f"  -> [7/7] ğŸ”— æ­£åœ¨è®¡ç®—æˆå¯¹ç¥ç»å…ƒåŒæ­¥æ€§ (Pairwise Sync) ä»¥åŠéªŒè¯å…±å‘æ”¾ç³»æ•°...")
    sync_values = calculate_pairwise_sync(pyr_spike_times, T_total)
    validation_data = calculate_cofiring_validation(spike_count_matrix, weight_vectors)

    summary = {"Mouse_ID": mouse_id, "Avg_Assembly_Strength": avg_strength, "Avg_Assembly_Frequency_Hz": avg_frequency}
    sync_df = pd.DataFrame({"Mouse_ID": mouse_id, "Normalized_Sync_Value": sync_values})
    validation_df = pd.DataFrame(validation_data)
    if not validation_df.empty:
        validation_df["Mouse_ID"] = mouse_id

    print(f"âœ… æ–‡ä»¶ {mouse_id} åˆ†ææµç¨‹å…¨éƒ¨å®Œæˆï¼")
    return summary, sync_df, validation_df


# =============================================================================
# Main Execution Block
# =============================================================================

def main():
    root = tk.Tk()
    root.withdraw()

    print("ğŸŒŸ æ¬¢è¿ä½¿ç”¨ mPFC ç¥ç»å…ƒé›†ç¾¤åˆ†æå·¥å…·ï¼")
    
    input_dir = filedialog.askdirectory(title="è¯·é€‰æ‹©åŒ…å«åŸå§‹æ•°æ® (.xlsx) çš„æ–‡ä»¶å¤¹")
    if not input_dir:
        print("âŒ æœªé€‰æ‹©è¾“å…¥æ–‡ä»¶å¤¹ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    output_dir = filedialog.askdirectory(title="è¯·é€‰æ‹©ä¿å­˜ç»“æœçš„æ–‡ä»¶å¤¹")
    if not output_dir:
        print("âŒ æœªé€‰æ‹©è¾“å‡ºæ–‡ä»¶å¤¹ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    all_files = [f for f in os.listdir(input_dir) if f.endswith(".xlsx") and not f.startswith("~")]
    if not all_files:
        print("âŒ åœ¨æ‰€é€‰æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„ .xlsx æ–‡ä»¶ã€‚")
        return

    print(f"\nğŸ“ æˆåŠŸæ‰¾åˆ°ä»¥ä¸‹ {len(all_files)} ä¸ªæ–‡ä»¶å‡†å¤‡åˆ†æ:")
    for i, fname in enumerate(all_files):
        print(f"  [{i + 1}] {fname}")

    processing_order = []
    while not processing_order:
        choice = input("\nâŒ¨ï¸ è¯·é€‰æ‹©æ–‡ä»¶åˆ†æé¡ºåº (è¾“å…¥æ•°å­—å¹¶æŒ‰å›è½¦):\n"
                       "  1: å‡åºæ’åˆ— (é»˜è®¤)\n"
                       "  2: é™åºæ’åˆ—\n"
                       "  3: è‡ªå®šä¹‰é¡ºåº\n"
                       "ğŸ‘‰ ä½ çš„é€‰æ‹©: ")
        if choice == '1' or choice == '':
            processing_order = sorted(all_files)
        elif choice == '2':
            processing_order = sorted(all_files, reverse=True)
        elif choice == '3':
            custom_order_str = input(f"è¯·è¾“å…¥æ–‡ä»¶ç¼–å· (1-{len(all_files)})ï¼Œä»¥è‹±æ–‡é€—å·åˆ†éš” (ä¾‹å¦‚: 3,1,2): ")
            try:
                order_indices = [int(x.strip()) - 1 for x in custom_order_str.split(',')]
                # More robust check for custom order input
                if len(order_indices) == len(all_files) and len(set(order_indices)) == len(all_files) and all(
                        0 <= i < len(all_files) for i in order_indices):
                    processing_order = [all_files[i] for i in order_indices]
                else:
                    print("âš ï¸ é”™è¯¯: è¾“å…¥çš„ç¼–å·æœ‰é‡å¤ã€è¶…å‡ºèŒƒå›´æˆ–æ•°é‡ä¸æ­£ç¡®ï¼Œè¯·é‡è¯•ã€‚")
            except (ValueError, IndexError):
                print("âš ï¸ é”™è¯¯: è¯·ç¡®ä¿è¾“å…¥çš„æ˜¯ä»¥é€—å·åˆ†éš”çš„æœ‰æ•ˆæ•°å­—ï¼Œè¯·é‡è¯•ã€‚")
        else:
            print("âš ï¸ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥1, 2, æˆ– 3ã€‚")

    print("\nğŸš€ å³å°†æŒ‰ä»¥ä¸‹é¡ºåºå¼€å¯åˆ†æç‹‚é£™æ¨¡å¼:")
    for fname in processing_order:
        print(f"  ğŸ”œ {fname}")
    
    all_summary_data, all_sync_dfs, all_validation_dfs = [], [], []

    for filename in processing_order:
        file_path = os.path.join(input_dir, filename)
        summary, sync_df, validation_df = process_single_file(file_path, save_fig5b_data=True, output_dir=output_dir)

        if summary:
            all_summary_data.append(summary)
            all_sync_dfs.append(sync_df)
            if validation_df is not None and not validation_df.empty:
                all_validation_dfs.append(validation_df)

    if not all_summary_data:
        print("\n åˆ†æç»“æŸï¼Œä½†æœªèƒ½ç”Ÿæˆä»»ä½•æœ‰æ•ˆçš„æ±‡æ€»æ•°æ®ã€‚è¯·æ£€æŸ¥åŸå§‹æ–‡ä»¶ã€‚")
        return

    print("\n==================================================")
    print("ğŸ’¾ æ­£åœ¨æ•´åˆæ‰€æœ‰ç»“æœå¹¶å†™å…¥æœ€ç»ˆçš„ Excel æ–‡ä»¶...")
    final_summary_df = pd.DataFrame(all_summary_data)
    final_sync_df = pd.concat(all_sync_dfs, ignore_index=True) if all_sync_dfs else pd.DataFrame()
    final_validation_df = pd.concat(all_validation_dfs, ignore_index=True) if all_validation_dfs else pd.DataFrame()

    output_path = os.path.join(output_dir, "analysis_results_final.xlsx")

    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        final_summary_df.to_excel(writer, sheet_name="Assembly_Summary", index=False)
        final_sync_df.to_excel(writer, sheet_name="Pairwise_Sync_Data", index=False)
        if not final_validation_df.empty:
            final_validation_df.to_excel(writer, sheet_name="Cofiring_Validation_Data", index=False)

    print(f"ğŸ‰ æ­å–œç»´å¡ï¼æ‰€æœ‰åˆ†æå‡å·²é¡ºåˆ©å®Œæˆï¼")
    print(f"ğŸ“„ æœ€ç»ˆç»“æœæ€»è¡¨å·²ä¿å­˜è‡³: {output_path}")
    print("==================================================\n")


if __name__ == "__main__":
    main()